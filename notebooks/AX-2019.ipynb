{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AX-2019.ipynb","version":"0.3.2","provenance":[{"file_id":"1YdfUaUlOEPHsyo3gr_xOZhU-HtPEX5EW","timestamp":1549467931691}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"-W5cIJOGm9F7","colab_type":"text"},"cell_type":"markdown","source":["# Assessed Exercise for Deep Learning (M)\n","\n","This exercise must be submitted as a colab notebook. Deadline  Monday the 4th of March, 15:00.\n","\n"]},{"metadata":{"id":"0clTC4n9opVP","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","You will create a classifier and test it on a collection of images for a new task. While you are welcome to build a full network from scratch, most of you will not have sufficient access to the data and compuational power required, so you are welcome to provide a solution based on transfer learning from a pre-trained network, adapted to your new task. \n","\n","The Keras documentation has some examples of pretrained networks which are easy to integrate, see for example  their ResNet50 example [https://keras.io/applications/#classify-imagenet-classes-with-resnet50](https://keras.io/applications/#classify-imagenet-classes-with-resnet50), or the InceptionV3 network  [https://keras.io/applications/#fine-tune-inceptionv3-on-a-new-set-of-classes](https://keras.io/applications/#fine-tune-inceptionv3-on-a-new-set-of-classes). If you use the pretrained model, a good way to start is by freezing all layers up to the last layer before the output. Adapt the output layer to fit your classification problem. You might then unfreeze some earlier layers for further fine tuning.\n","\n","You will need to create a training set (at least 100 images per class, potentially classifying e.g type of location, activity. If you are training a full network from scratch you would need orders of magnitude more, but this will work for transfer learning on an existing network). It might be sensible to start off testing and demonstrating your approach by using an existing dataset e.g. \n","the [flowers one](http://www.robots.ox.ac.uk/~vgg/data/flowers/102/). You can find other interesting datasets at:\n","*   [http://deeplearning.net/datasets](http://deeplearning.net/datasets/)  \n","*   [UCI ML collection](https://archive.ics.uci.edu/ml/datasets.html)\n","*   [https://www.visualdata.io](https://www.visualdata.io)\n","*   [https://ai.google/tools/datasets](https://ai.google/tools/datasets/ )\n","\n","Students who put more effort into creating and analysing an interesting dataset will tend to do better in marks for sections 1. & 2. below.\n","\n","Write a pre-processing step that will resize and crop the images to the right size ((224, 224) is default for ResNet50 and (299,299) is the default for Inception), and consider how you can apply data augmentation techniques to your new dataset, and design appropriate pre-processing functions.\n","\n","In your submission you should have the following structure (share of the AX marks given in brackets at the end of each part):\n","\n","1.   Analysis of the problem. (15%)\n","2.   Visualisation and analysis of the data type, quality and  class distributions. You may want to design some data augmentation in your system. (20%)\n","3.   Creation of multiple candidate network architectures. Include your justification of the design decisions. You should inlcude one very simple baseline model (e.g. a linear model, or a simple two layer Densely connected model). (15%)\n","4.   Training. This should include code for hyperparameter search, regularisation methods. (15%)\n","5.   Empirical evaluation of performance, and potentially visualisation and  analysis of the trained network. This should make good use of graphs and tables of results, confusion matrices etc to represent the relative performance of the different models.  Explain why you chose the metrics you use. (20%)\n","6.   Report on the performance, discussing the suitability of the final network for use. (15%)\n","\n","For each of the design decisions, make sure you describe in detail the motivation behind them. \n","\n","**Submission process**\n","\n","You should submit the colab notebook with *all* code needed to run your model and all visualisations of results in place (I don't want to have to run 80 projects :-) ). This exercise must be submitted as a colab notebook. Deadline Monday the 4th of March, 15:00. If you have your own training data, make sure that any links to that are accessible by 3rd parties (me - I won't share the links with anyone else).\n","\n","Share the Colab link (click on Share on top right of the Colab notebook, then 'Get shareable link') with me by e-mail [Roderick.Murray-Smith@glasgow.ac.uk](mailto:Roderick.Murray-Smith@glasgow.ac.uk?subject=Deep Learning AX 2019) and make sure that the Subject of the e-mail is *exactly* `'Deep Learning AX 2019'` (automatically generated if you click on the e-mail hyperlink above)"]},{"metadata":{"id":"XOpdUtkApTp3","colab_type":"text"},"cell_type":"markdown","source":["# Some code snippets which might be useful to get you started"]},{"metadata":{"id":"_JvVJdpidmBl","colab_type":"code","cellView":"both","outputId":"4fac79c1-568a-4cb3-c07b-3075a3b9e696","executionInfo":{"status":"ok","timestamp":1549987114590,"user_tz":0,"elapsed":1981,"user":{"displayName":"Andrew Mackay","photoUrl":"https://lh3.googleusercontent.com/-24hiGmdxZDE/AAAAAAAAAAI/AAAAAAAAL_I/RW7nqM11LkM/s64/photo.jpg","userId":"06804410358976473893"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["#@title Run this to import the right things\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import collections\n","import math\n","import os\n","import random\n","from six.moves import urllib\n","import io\n","import shutil\n","\n","from IPython.display import clear_output, Image, display, HTML\n","\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import sklearn.metrics as sk_metrics\n","import time\n","from tensorflow.python.keras.applications.resnet50 import preprocess_input\n","from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n","\n","from keras.applications.inception_v3 import InceptionV3\n","from keras.applications.resnet50 import preprocess_input, decode_predictions, ResNet50\n","from keras.preprocessing import image\n","from keras import regularizers\n","from keras.models import Model\n","from keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout, Flatten\n","from keras import backend as K"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"wss7V4SJjo4v","colab_type":"text"},"cell_type":"markdown","source":["# Image Processing"]},{"metadata":{"id":"Xpp4-gnWjs_I","colab_type":"text"},"cell_type":"markdown","source":["## Extracting frames from videos and resizing to _x_\n","@TODO enter size"]},{"metadata":{"id":"PO-tgk0Sk_0S","colab_type":"code","colab":{}},"cell_type":"code","source":["TARGET_IMAGE_SHAPE = (224, 224)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"j_sPPVISbXdk","colab_type":"code","colab":{}},"cell_type":"code","source":["import glob\n","import cv2\n","from skimage.transform import resize"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hs_eIr6seO-O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":146},"outputId":"29582ad2-8748-421f-cefa-2d65bbd49429","executionInfo":{"status":"ok","timestamp":1549989838785,"user_tz":0,"elapsed":18881,"user":{"displayName":"Andrew Mackay","photoUrl":"https://lh3.googleusercontent.com/-24hiGmdxZDE/AAAAAAAAAAI/AAAAAAAAL_I/RW7nqM11LkM/s64/photo.jpg","userId":"06804410358976473893"}}},"cell_type":"code","source":["path = os.getcwd()\n","BASE_PATH = \"\" # set manually if not using CoLab\n","print(path)\n","if path == '/content' or path == '/content/gdrive/My Drive/building_classifier':\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","    BASE_PATH = '/content/gdrive/My Drive/building_classifier/'\n","    os.chdir('/content/gdrive/My Drive/building_classifier/')\n","    \n","else:\n","    print(\"Please set BASE_PATH manually\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"abLXiFtWciHv","colab_type":"code","colab":{}},"cell_type":"code","source":["VIDEO_PATH = BASE_PATH + 'data/raw/12_02_2019/trimmed/'\n","INTERIM_PATH = BASE_PATH + 'data/interim/extracted_frames/'\n","\n","if not os.path.exists(INTERIM_PATH):\n","    os.makedirs(INTERIM_PATH)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xZJ4H-16h1S4","colab_type":"code","colab":{}},"cell_type":"code","source":["# assumes landscape ratio\n","def crop_to_square(image):\n","    height = image.shape[0]\n","    width_centre = image.shape[1]/2\n","    start_width = int(width_centre - (height/2))\n","    end_width = int(width_centre + (height/2))\n","    return image[:,start_width:end_width,:]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Z8l7OWFYiWlS","colab_type":"code","colab":{}},"cell_type":"code","source":["type_counts = {\n","    \"boyd\": 0,\n","    \"adam\": 0,\n","    \"reading\": 0,\n","    \"main\": 0,\n","    \"james\": 0,\n","    \"main\": 0,\n","    \"rankine\": 0\n","}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LDw5O0gKdhC2","colab_type":"code","outputId":"bdf25672-7045-451a-8de7-a3656e9643c0","executionInfo":{"status":"ok","timestamp":1549990462831,"user_tz":0,"elapsed":269193,"user":{"displayName":"Andrew Mackay","photoUrl":"https://lh3.googleusercontent.com/-24hiGmdxZDE/AAAAAAAAAAI/AAAAAAAAL_I/RW7nqM11LkM/s64/photo.jpg","userId":"06804410358976473893"}},"colab":{"base_uri":"https://localhost:8080/","height":689}},"cell_type":"code","source":["total = 0\n","for path in glob.glob(VIDEO_PATH + \"/*.mp4\"):\n","    split_path = path.split(\"/\")\n","    file_name = split_path[-1]\n","    split_file_name = file_name.split('_')\n","    building = split_file_name[0] \n","    vidcap = cv2.VideoCapture(path)\n","    success = 1\n","    count = 0\n","    while success:\n","        success, image = vidcap.read()  \n","        if success:\n","            image = crop_to_square(image)\n","            image = resize(image, TARGET_IMAGE_SHAPE)\n","            count += 1\n","    print(building, count)\n","    total += count\n","print(\"Total:\", total)\n","        \n","#             plt.imshow(image)\n","#             plt.show()\n","#             type_counts[building] += 1\n","#             cv2.imwrite(\"frame%d.jpg\" % count, image)     # save frame as JPEG file  \n","#     break\n","\n","            \n","#         else:\n","#             break\n","#         count += 1"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n","  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"],"name":"stderr"},{"output_type":"stream","text":["adam 212\n","reading 105\n","reading 152\n","reading 127\n","reading 235\n","main 237\n","main 240\n","main 208\n","main 263\n","main 195\n","main 224\n","main 233\n","main 133\n","james 193\n","james 208\n","james 238\n","james 273\n","james 176\n","rankine 142\n","rankine 162\n","rankine 233\n","rankine 160\n","boyd 294\n","boyd 359\n","boyd 349\n","boyd 283\n","boyd 141\n","adam 344\n","adam 259\n","adam 222\n","adam 291\n","adam 274\n","reading 129\n","main 199\n","Total: 7493\n"],"name":"stdout"}]},{"metadata":{"id":"rO0UrIo1OXpA","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Load the Flowers data set and split into test & train sets [RUN ME]\n","FLOWERS_DIR = './flower_photos'\n","TRAIN_FRACTION = 0.8\n","RANDOM_SEED = 2018\n","\n","\n","def download_images():\n","  \"\"\"If the images aren't already downloaded, save them to FLOWERS_DIR.\"\"\"\n","  if not os.path.exists(FLOWERS_DIR):\n","    DOWNLOAD_URL = 'http://download.tensorflow.org/example_images/flower_photos.tgz'\n","    print('Downloading flower images from %s...' % DOWNLOAD_URL)\n","    urllib.request.urlretrieve(DOWNLOAD_URL, 'flower_photos.tgz')\n","    !tar xfz flower_photos.tgz\n","  print('Flower photos are located in %s' % FLOWERS_DIR)\n","  \n","download_images()\n","\n","!rm ./flower_photos/LICENSE.txt\n","if not os.path.exists('./flowers'):\n","    os.mkdir('./flowers')\n","\n","\n","image_dir = FLOWERS_DIR\n","from sklearn.model_selection import train_test_split\n","\n","# Read all flower images (.jpg) from a folder\n","# The function returns both the path of the flower image and the corresponding label\n","# which is defined by the name of the foler in which the image is\n","def read_images_from_dir(base_dir, folder):\n","    path_folder = os.path.join(image_dir, folder)\n","    files_directory = os.listdir(path_folder)\n","    \n","    labels = []\n","    images = []\n","    for file in files_directory:\n","        if file.endswith('.jpg'):\n","            labels.append(folder)\n","            images.append(os.path.join(path_folder, file))\n","    return labels, images\n","\n","def read_images(base_dir):\n","    labels = []\n","    images = []\n","    folders = os.listdir(image_dir)\n","    for folder in folders:\n","        labels_folder, images_folder = read_images_from_dir(base_dir, folder)\n","        labels.extend(labels_folder)\n","        images.extend(images_folder)\n","    return labels, images\n","\n","labels, images = read_images(image_dir)\n","\n","\n","# Split the data into test and training sets\n","images_train, images_test, labels_train, labels_test = train_test_split(images, labels, test_size=0.3, random_state=8, stratify=labels)\n","\n","# Create folders for test and training that can be passed to Keras Generators\n","train_folder = './flowers/output/train'\n","test_folder  = './flowers/output/test'\n","output_folder = './flowers/output'\n","\n","def create_output_folders():\n","    if not os.path.exists(output_folder):\n","        print('Creating output directories')\n","        os.mkdir(output_folder)\n","        if not os.path.exists(train_folder):\n","            os.mkdir(train_folder)\n","            for label in set(labels):\n","                os.mkdir(train_folder + '/' + label)\n","        if not os.path.exists(test_folder):\n","            os.mkdir(test_folder)\n","            for label in set(labels):\n","                os.mkdir(test_folder + '/' + label)\n","\n","def copy_files_to_train_and_validation_folders():            \n","    print('Copy training files to directory')\n","    for index, value in enumerate(images_train):\n","        dest = os.path.join(train_folder, labels_train[index])\n","        shutil.copy(value, dest)\n","\n","    print('Copy test files to directory')        \n","    for index, value in enumerate(images_test):\n","        shutil.copy(value, test_folder + '/' + labels_test[index])\n","        \n","create_output_folders()\n","copy_files_to_train_and_validation_folders()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MWcbdolPpGf8","colab_type":"text"},"cell_type":"markdown","source":["We now create some data generators for working with the flowers data"]},{"metadata":{"id":"QwWW3_wiGiHd","colab_type":"code","colab":{}},"cell_type":"code","source":["#images_train, images_test, labels_train, labels_test = train_test_split(images, labels, test_size=0.3, random_state=8, stratify=labels)\n","\n","image_size = 224\n","data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n","\n","train_generator      = data_generator.flow_from_directory(train_folder, target_size=(image_size, image_size), batch_size=24, class_mode='categorical')\n","validation_generator = data_generator.flow_from_directory(test_folder, target_size=(image_size, image_size), batch_size=24, class_mode='categorical')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tYoRv1DvpMto","colab_type":"text"},"cell_type":"markdown","source":["Adapt the code below to work with the generators, then proceed with the experiments!"]},{"metadata":{"id":"qvrp_RFHLXVu","colab_type":"code","colab":{}},"cell_type":"code","source":["# create the base pre-trained model\n","#base_model = InceptionV3(weights='imagenet', include_top=False)\n","base_model = ResNet50(weights='imagenet', include_top=False)\n","\n","lamb = 0.001\n","\n","# add a global spatial average pooling layer\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","\n","# and a logistic layer -- let's say we have 200 classes\n","predictions = Dense(YOUR NUMBER OF CLASSES, activation='softmax')(x)\n","\n","# this is the model we will train\n","model = Model(inputs=base_model.input, outputs=predictions)\n","\n","# first: train only the top layers (which were randomly initialized)\n","# i.e. freeze all convolutional InceptionV3 layers\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# compile the model (should be done *after* setting layers to non-trainable)\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'] )\n","\n","# train the model on the new data for a few epochs\n","model.fit_generator(FILL IN YOUR DETAILS...)\n","\n","# at this point, the top layers are well trained and we can start fine-tuning\n","# convolutional layers from inception V3. We will freeze the bottom N layers\n","# and train the remaining top layers.\n","\n","# let's visualize layer names and layer indices to see how many layers\n","# we should freeze:\n","for i, layer in enumerate(base_model.layers):\n","   print(i, layer.name)\n","\n","# we chose to train the top 2 inception blocks, i.e. we will freeze\n","# the first 249 layers and unfreeze the rest:\n","for layer in model.layers[:249]:\n","   layer.trainable = False\n","for layer in model.layers[249:]:\n","   layer.trainable = True\n","\n","# we need to recompile the model for these modifications to take effect\n","# we use SGD with a low learning rate\n","from keras.optimizers import SGD\n","model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# we train our model again (this time fine-tuning the top 2 inception blocks\n","# alongside the top Dense layers\n","model.fit_generator(FILL IN YOUR DETAILS...)"],"execution_count":0,"outputs":[]}]}